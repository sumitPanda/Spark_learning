{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668acf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Softwares\\\\Spark_installtion\\\\spark-3.4.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good practice to have the libraries set here. \n",
    "\n",
    "import findspark\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrameWriterV2\n",
    "#Lets check SPARK, where exactly it is installed.\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85aedea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DragonWarrior:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DBConnection</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1de60f9c280>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets initiate spark session.  I am using Spark 3.4.1 so  there is no need to start the spark context. \n",
    "#It is available in the system itself. \n",
    "\n",
    "spark = (\n",
    "            SparkSession\n",
    "                .builder\n",
    "                .appName(\"DBConnection\" ) # any name for yourself. \n",
    "                .master(\"local[*]\") # Why this? well, we are running in local mode. \n",
    "                .config(\"spark.jars\",\"./JARS/sqljdbc_12.4/enu/jars/spark-mssql-connector_2.12-1.2.0.jar\") # this is required to read the data from SQL SERVER\n",
    "                .getOrCreate() \n",
    "        )\n",
    "\n",
    "spark # lets just check if it is set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###This is required when I have to stop the spark and restart the sessions. #####\n",
    "\n",
    "#sc = spark.sparkContext\n",
    "#sc.stop()\n",
    "#spark.stop()\n",
    "#print(\"server stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7382924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_name = \"jdbc:sqlserver://localhost:1433\"\n",
    "database_name = \"SALES50M\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";encrypt=true;trustServerCertificate=true;\"\n",
    "table_name = \"[SALES50M].[dbo].[SALES]\"\n",
    "username = \"Root\"\n",
    "password = \"Root@123\" # Please specify password here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3295b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added the JAR file in the system ( wherever spark is installed) or add it in the dependecy in the DatabRicks library\n",
    "\n",
    "Sales_DF = spark.read \\\n",
    "        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password)\\\n",
    "        .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b88e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ID=44896043, ORDERID=8977488, ORDERDETAILID=44896048, DATE_=datetime.datetime(2023, 3, 30, 0, 0), USERID=64416, USERNAME_='cey_dikcinar@fakehotmail.com', NAMESURNAME='Ceylan Cansu DİKÇINAR', STATUS_=1, ITEMID=10790, ITEMCODE='4704', ITEMNAME='SERVE CREATIVE VERSATIL KALEM 0.7', AMOUNT=4.0, UNITPRICE=7.2, PRICE=7.5, TOTALPRICE=30.0, CATEGORY1='EV', CATEGORY2='KITAP-DERGI-KIRTASIYE', CATEGORY3='KIRTASIYE', CATEGORY4='KIRTASIYE GERECLERI', BRAND='KIRTASIYELER', USERGENDER='K', USERBIRTHDATE=datetime.date(1953, 11, 28), REGION='Marmara', CITY='İstanbul', TOWN='ÇATALCA', DISTRICT='ÖRCÜNLÜ KÖYÜ', ADDRESSTEXT='ÖRCÜNLÜ KÖYÜ MAH. ALPTEKİN SOKAK  34540  ÇATALCA/İSTANBUL', ADDRESSID=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sales_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "887b9e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'ORDERID',\n",
       " 'ORDERDETAILID',\n",
       " 'DATE_',\n",
       " 'USERID',\n",
       " 'USERNAME_',\n",
       " 'NAMESURNAME',\n",
       " 'STATUS_',\n",
       " 'ITEMID',\n",
       " 'ITEMCODE',\n",
       " 'ITEMNAME',\n",
       " 'AMOUNT',\n",
       " 'UNITPRICE',\n",
       " 'PRICE',\n",
       " 'TOTALPRICE',\n",
       " 'CATEGORY1',\n",
       " 'CATEGORY2',\n",
       " 'CATEGORY3',\n",
       " 'CATEGORY4',\n",
       " 'BRAND',\n",
       " 'USERGENDER',\n",
       " 'USERBIRTHDATE',\n",
       " 'REGION',\n",
       " 'CITY',\n",
       " 'TOWN',\n",
       " 'DISTRICT',\n",
       " 'ADDRESSTEXT',\n",
       " 'ADDRESSID']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sales_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c941eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCT_DF = Sales_DF.select(\n",
    " 'ITEMID',\n",
    " 'ITEMCODE',\n",
    " 'ITEMNAME',\n",
    " 'UNITPRICE',\n",
    " 'CATEGORY1',\n",
    " 'CATEGORY2',\n",
    " 'CATEGORY3',\n",
    " 'CATEGORY4',\n",
    " 'BRAND').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a87e1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " PRODUCT_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96e9cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes you will see error if you use the complete stream. So just JDBC works in the higher versions of spark. \n",
    "PRODUCT_DF.write \\\n",
    "    .format(\"jdbc\") \\ \n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"ProductMaster_STG\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72180c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100217"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets break the table into smaller tables to have the smaller set. lets have the users here . \n",
    "User_DF = Sales_DF.select(\n",
    " 'USERID',\n",
    " 'USERNAME_',\n",
    " 'NAMESURNAME',\n",
    " 'USERGENDER',\n",
    " 'USERBIRTHDATE').distinct()\n",
    "\n",
    "User_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "604ebb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes you will see error if you use the complete stream. So just JDBC works in the higher versions of spark. \n",
    "User_DF.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"UserMaster_STG\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17b0bbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93401"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Address_DF = Sales_DF.select(\n",
    "'ADDRESSID',\n",
    " 'REGION',\n",
    " 'CITY',\n",
    " 'TOWN',\n",
    " 'DISTRICT',\n",
    " 'ADDRESSTEXT'\n",
    " ).distinct()\n",
    "\n",
    "Address_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f861b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes you will see error if you use the complete com.microsoft.sqlserver.jdbc.spark in the format deck.\n",
    "#So just JDBC works in the higher versions of spark in the format space. \n",
    "\n",
    "Address_DF.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"AddressMaster_STG\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddca6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sales_fact_DF= Sales_DF.select(\n",
    "    'ID',\n",
    " 'ORDERID',\n",
    " 'ORDERDETAILID',\n",
    " 'DATE_',\n",
    " 'USERID',\n",
    " 'STATUS_',\n",
    " 'ITEMID',\n",
    " 'AMOUNT',\n",
    " 'TOTALPRICE',\n",
    " 'ADDRESSID'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6f3760",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'month' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Sales_fact_DF \u001b[38;5;241m=\u001b[39m Sales_fact_DF\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mmonth\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      2\u001b[0m Sales_fact_DF \u001b[38;5;241m=\u001b[39m Sales_fact_DF\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m,year(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'month' is not defined"
     ]
    }
   ],
   "source": [
    "Sales_fact_DF = Sales_fact_DF.withColumn(\"Month\",month(\"DATE_\"))\n",
    "Sales_fact_DF = Sales_fact_DF.withColumn(\"Year\",year(\"DATE_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3df2c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes you will see error if you use the complete com.microsoft.sqlserver.jdbc.spark in the format deck.\n",
    "#So just JDBC works in the higher versions of spark in the format space. \n",
    "\n",
    "Sales_fact_DF.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", \"Sales_Fact_STG\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b23fb",
   "metadata": {},
   "source": [
    "#<B>LETS WORK on some new libraries. </B>#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec32e5f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriterV2' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m Sales_fact_DF2 \u001b[38;5;241m=\u001b[39m DataFrameWriterV2(Sales_fact_DF, table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSales_Fact\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#lets write the data into parquet files in the folder.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m Sales_fact_DF2\u001b[38;5;241m=\u001b[39m\u001b[43mSales_fact_DF2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m Sales_fact_DF2\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m      9\u001b[0m \u001b[38;5;241m.\u001b[39mpartitionedBy(months(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE_\u001b[39m\u001b[38;5;124m\"\u001b[39m),years(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\\\n\u001b[0;32m     10\u001b[0m \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet.writer.compaction.code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNAPPY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m     11\u001b[0m \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet.writer.compaction.threshold.bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\\\n\u001b[0;32m     12\u001b[0m \u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Data sets/Turkish_Market_Sales_Dataset/Spark_storage/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrameWriterV2' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "#DATAFRAMEWRITERV2\n",
    "#lets divide the data by partition \n",
    "Sales_fact_DF2 = DataFrameWriterV2(Sales_fact_DF, table=\"Sales_Fact\")\n",
    "\n",
    "#lets write the data into parquet files in the folder.\n",
    "Sales_fact_DF2=Sales_fact_DF2.write.format(\"parquet\")\n",
    "\n",
    "Sales_fact_DF2.mode(\"overwrite\")\\\n",
    ".partitionedBy(months(\"DATE_\"),years(\"DATE_\"))\\\n",
    ".option(\"parquet.writer.compaction.code\", \"SNAPPY\")\\\n",
    ".option(\"parquet.writer.compaction.threshold.bytes\", 2 * 1024 * 1024)\\\n",
    ".save(\"D:/Data sets/Turkish_Market_Sales_Dataset/Spark_storage/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91da824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriterV2 at 0x1de62793370>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATAFRAMEWRITERV2\n",
    "\n",
    "Sales_fact_DF2 = DataFrameWriterV2(Sales_fact_DF, table=\"Sales_Fact\")\n",
    "\n",
    "#lets write the data into parquet files in the folder.\n",
    "Sales_fact_DF2.option(\"Formtat\",\"parquet\")\\\n",
    ".option(\"writefile\",\"D:/Data sets/Turkish_Market_Sales_Dataset/Spark_storage/V2/\")\\\n",
    ".partitionedBy(\"DATE_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5468a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server stopped\n"
     ]
    }
   ],
   "source": [
    "###This is required when I have to stop the spark and restart the sessions. #####\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.stop()\n",
    "spark.stop()\n",
    "print(\"server stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
